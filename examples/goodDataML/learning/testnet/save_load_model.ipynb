{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to transmit Machine Learning Model in GoodData System\n",
    "\n",
    "### Option 1  -- Save model to file\n",
    "#### Submit Query\n",
    "1. Query Customer need to define a model locally and save the model to a file with ```torch.save()```.\n",
    "2. Query Customer uploads the file through UI, UI (Javascript) should read the file, convert the file to byte steam and send it to the GDS Service using GRPC ```SubmitQuery```.\n",
    "3. GRPC will store the serialized model into DB without deserializing it.\n",
    "4. Once DO gets notification from the blockchain, it should query GDS Service to get the byte steam using GRPC ```GetQueryInfo```.\n",
    "5. Once DO gets the byte steam, it will store it to a temp file with timestamp and query uuid(See example above).\n",
    "6. DO calls ```torch.load()``` to load the model from the temp file and start training.\n",
    "\n",
    "#### Query Completed\n",
    "1. DO stores trained model's ```state_dict``` to temp file.\n",
    "2. DO reads temp file as byte stream and call ```QueryCompleted``` to send query to GDS Service. **Delete all temp files related to this query**\n",
    "3. GDS Service gets the result and dowa consistency check. (Need to call python script to compare model results from different DOs).\n",
    "4. QC calla ```GetQueryExecutionInfo``` once it knows query completed. Download the result into a file. Load the file and get the model's ```state_dict```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Query Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Save Model to temp file\n",
    "filename_start = './model_uuid_start'+ str(time.time())+'.pt'\n",
    "torch.save(model, filename_start, pickle_protocol=2)\n",
    "\n",
    "# .... Sending and receiving with GRPC.......\n",
    "\n",
    "# Load Model from file\n",
    "model_new_start = torch.load(filename_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_uuid_start1597822720.728288.pt\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n"
     ]
    }
   ],
   "source": [
    "print(filename_start)\n",
    "print(model.state_dict())\n",
    "print(model_new_start.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Completed Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0267, -0.4148,  0.3465,  0.0986, -0.3573]])), ('fc1.bias', tensor([0.4338]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model state_dict to temp file\n",
    "filename_completed = './model_uuid_completed'+ str(time.time())+'.pt'\n",
    "torch.save(model.state_dict(), filename_completed)\n",
    "\n",
    "# .... Sending and receiving with GRPC.......\n",
    "\n",
    "# Load Model state_dict from file\n",
    "model_new_completed = Net()\n",
    "print(model_new_completed.state_dict())\n",
    "model_new_completed.load_state_dict(torch.load(filename_completed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_uuid_completed1597822720.771007.pt\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n"
     ]
    }
   ],
   "source": [
    "print(filename_completed)\n",
    "print(model.state_dict())\n",
    "print(model_new_completed.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2  -- Save model in memory (Recommended)\n",
    "#### Submit Query\n",
    "1. Query Customer need to define a model locally and save the model to a file with ```pickle.dumps()```.\n",
    "2. Query Customer uploads the file through UI, UI (Javascript) should read the file, converts the file to byte steam and sends it to the GDS Service using GRPC ```SubmitQuery```.\n",
    "3. GRPC will store the serialized model into DB without deserializing it.\n",
    "4. Once DO gets notification from the blockchain, it should query GDS Service to get the byte steam using GRPC ```GetQueryInfo```.\n",
    "5. Once DO gets the byte steam, it will load it to memory via ```pickle.loads()```\n",
    "\n",
    "#### Query Completed\n",
    "1. DO calls ```QueryCompleted``` to send query to GDS Service with trained model's ```state_dict```.\n",
    "3. GDS Service gets the result and do consistency check. (Need to call python script to compare model results from different DOs).\n",
    "4. QC calls ```GetQueryExecutionInfo``` once it knows query completed. Download the result into a file. Load the file and get the model's ```state_dict```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Query Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "buffer = pickle.dumps(model, protocol=2)\n",
    "\n",
    "# .... Sending and receiving with GRPC.......\n",
    "\n",
    "new_model2 = pickle.loads(buffer)\n",
    "\n",
    "## Even better but I have some problems to make it\n",
    "# import io\n",
    "# buffer = io.BytesIO()\n",
    "# torch.save(x, buffer, pickle_protocol=2)\n",
    "# model2 = torch.load(buffer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n",
    "print(new_model2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Completed Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0977, -0.0238,  0.1705,  0.2426, -0.2601]])), ('fc1.bias', tensor([-0.0855]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model state_dict to bytes object\n",
    "query_completed_buffer = pickle.dumps(model.state_dict(), protocol=2)\n",
    "\n",
    "# .... Sending and receiving with GRPC.......\n",
    "\n",
    "# Load Model state_dict from bytes object\n",
    "model_new_completed2 = Net()\n",
    "print(model_new_completed2.state_dict())\n",
    "model_new_completed2.load_state_dict(pickle.loads(query_completed_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.3154, -0.3879,  0.3616, -0.1143,  0.2432]])), ('fc1.bias', tensor([0.3548]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n",
    "print(model_new_completed2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Future work\n",
    "1. Encrypt the model with DO public key when sending to DO.\n",
    "2. Try to resolve the problem to use ```torch.save(x, io.BytesIO(), pickle_protocol=2)```\n",
    "\n",
    "### Open question\n",
    "1. Will Model be large enough to fit in memory?\n",
    "If yes, option 1 is preferred, otherwise can use option2.\n",
    "\n",
    "### Reference\n",
    "1. https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "2. https://docs.python.org/3/library/pickle.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
